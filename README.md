Pdf-Intelligent-Chat-With-History

An intelligent conversational RAG (Retrieval-Augmented Generation) chatbot that lets you:

Upload one or more PDF files

Ask questions in natural language

Get context-aware answers grounded in the PDFs

Maintain chat history per session

Use Groqâ€™s LLaMA 3.1 (8B Instant) for fast responses

Built with Streamlit, LangChain, ChromaDB, and Groq.

ğŸš€ Features
ğŸ“„ PDF Question Answering

Upload single or multiple PDFs

Load text using PyPDFLoader

Split documents into chunks using RecursiveCharacterTextSplitter

Create embeddings with HuggingFaceEmbeddings (all-MiniLM-L6-v2)

Store and search chunks using Chroma vector store

ğŸ’¬ Conversational RAG

Uses a history-aware retriever to rewrite the user query using chat history

Retrieves the most relevant chunks from your PDFs

Answers using the retrieved context

Keeps answers short and concise (â‰ˆ3 sentences, as per system prompt)

âš¡ Powered by Groq

Uses ChatGroq with model: llama-3.1-8b-instant

Very low latency â†’ good interactive experience in Streamlit

ğŸ§  Chat History per Session

Session-based chat memory using:

ChatMessageHistory

RunnableWithMessageHistory

Different session_id values â†’ separate conversations

ğŸ–¥ Streamlit UI

Input for Groq API key

File uploader for PDFs

Input for session ID

Text box for questions

Display of assistant answers and raw chat history

ğŸ“‚ Project Structure
project-root/
â”‚â”€â”€ app.py                 # Main Streamlit application
â”‚â”€â”€ requirements.txt       # Python dependencies
â”‚â”€â”€ .env                   # API keys & secrets (not committed)
â”‚â”€â”€ .gitignore             # Git ignore rules
â”‚â”€â”€ README.md              # Project documentation
â””â”€â”€ temp.pdf               # Temporary file created when processing uploads

ğŸ” Environment Variables

Create a .env file in the project root:

GROQ_API_KEY=your_groq_api_key_here
HF_TOKKEN=your_huggingface_token_here   # optional, matches your code variable name


Note: your code uses HF_TOKKEN (with double â€œKâ€), so the .env key must match exactly.

ğŸ›  Installation & Setup
1ï¸âƒ£ Clone the Repository
git clone https://github.com/vikrant-honbute/Pdf-Intelligent-Chat-With-History.git
cd Pdf-Intelligent-Chat-With-History

2ï¸âƒ£ Create a Virtual Environment
python -m venv venv


Activate it (Windows):

venv\Scripts\activate

3ï¸âƒ£ Install Dependencies
pip install -r requirements.txt

4ï¸âƒ£ Run the Application
streamlit run app.py


Then open the local URL shown in the terminal (usually http://localhost:8501).

ğŸ§© How It Works (High Level)

User enters Groq API key in the UI.

User uploads one or more PDF files.

PDFs are:

Loaded with PyPDFLoader

Split into overlapping chunks by RecursiveCharacterTextSplitter

Converted into embeddings using HuggingFaceEmbeddings

Stored in a Chroma vector store.

User asks a question.

A history-aware retriever:

Reads past messages (chat_history)

Reformulates the question into a standalone query if needed.

Relevant chunks are retrieved from Chroma.

ChatGroq (LLaMA 3.1 8B Instant) generates an answer using:

System prompt

Retrieved context

Chat history.

The answer and messages are stored in ChatMessageHistory for that session.

ğŸ’¡ Tech Stack
Component	Technology
UI	Streamlit
LLM	Groq â€“ LLaMA 3.1 8B Instant
Orchestration	LangChain (classic, core, community)
Vector Store	Chroma
Embeddings	HuggingFace MiniLM (all-MiniLM-L6-v2)
PDF Loader	PyPDFLoader
Memory	ChatMessageHistory + RunnableWithMessageHistory
ğŸ¤ Contributing

Issues, feature requests, and pull requests are welcome.

ğŸ“œ License

This project is licensed under the MIT License.

â­ Support

If you find this project useful, please consider giving it a â­ on GitHub.
