# ğŸ¤– Conversational RAG â€” PDF Chat With Memory  
A smart **Retrieval-Augmented Generation (RAG)** application that allows users to **upload PDFs and chat with them**, powered by **Groq LLM**, **LangChain Classic**, **ChromaDB**, and **Streamlit**.  
Supports **conversation history**, **question reformulation**, and **context-aware answers**.

---

## ğŸš€ Features

- ğŸ“‚ Upload and process **multiple PDF files**
- ğŸ” Automatic text splitting (500 chars, 200 overlap)
- ğŸ§  **HuggingFace Embeddings** (`all-MiniLM-L6-v2`)
- ğŸ—ƒï¸ **ChromaDB** vector database for fast retrieval
- ğŸ§µ **Session-based chat memory** using `ChatMessageHistory`
- ğŸ§© **History-aware retriever** (question rewriting)
- âš¡ Ultra-fast inference via **Groq LLM (llama-3.1-8b-instant)**
- ğŸ§  RAG pipeline using LangChain Classic chains
- ğŸ›ï¸ Clean, simple **Streamlit UI**

---

## ğŸ–¼ï¸ Demo / Screenshots  

## ğŸ“¸ Screenshot

<p align="center">
  <img src="<img width="1919" height="915" alt="UI" src="https://github.com/user-attachments/assets/e12ad591-0670-441b-a7a4-0fc9cca4ac63" />
" width="600" />
</p>





```
[ Conversational RAG â€” UI Screenshot Placeholder ]
```

---

## ğŸ§± Tech Stack

| Component | Technology |
|----------|------------|
| UI | Streamlit |
| LLM | Groq (llama-3.1-8b-instant) |
| Framework | LangChain Classic |
| Embeddings | HuggingFace MiniLM |
| Vector Store | ChromaDB |
| Document Loader | PyPDFLoader |
| Memory | ChatMessageHistory |

---

## ğŸ“¦ Installation

### 1ï¸âƒ£ Clone the repository
```bash
git clone https://github.com/yourusername/conversational-rag-pdf-chat.git
cd conversational-rag-pdf-chat
```

### 2ï¸âƒ£ Create a virtual environment
```bash
python -m venv venv
```

Activate:

**Windows**
```bash
venv\Scripts\activate
```

**Mac/Linux**
```bash
source venv/bin/activate
```

### 3ï¸âƒ£ Install dependencies
```bash
pip install -r requirements.txt
```

Suggested `requirements.txt`:

```
streamlit
python-dotenv
langchain-classic
langchain-core
langchain-community
langchain-chroma
langchain-groq
langchain-text-splitters
langchain-huggingface
pypdf
```

---

## ğŸ”‘ Environment Variables

Create `.env`:

```bash
HF_TOKKEN=your_huggingface_token_here
```

âš ï¸ Use exact name `HF_TOKKEN` â€” your code uses this spelling.

Groq API key is entered inside the UI.

---

## ğŸ“˜ How to Use

### Start the app
```bash
streamlit run app.py
```

### Steps
1. Enter your **Groq API key**.  
2. Enter **session ID** (default: `default_session`).  
3. Upload one or more **PDF files**.  
4. Ask questions related to the documents.  
5. The system uses:
   - Chat history  
   - Reformulated queries  
   - Retrieved chunks  
   - Groq LLM for final answers  

---

## ğŸ§© Core Architecture (Key Snippets)

### Embeddings
```python
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")
```

### Groq LLM
```python
llm = ChatGroq(groq_api_key=api_key, model_name="llama-3.1-8b-instant")
```

### History-Aware Retriever
```python
history_aware_retriever = create_history_aware_retriever(
    llm, retriever, contextualize_q_prompt
)
```

### Conversational RAG Chain
```python
conversational_rag_chain = RunnableWithMessageHistory(
    rag_chain,
    get_session_history,
    input_messages_key="input",
    history_messages_key="chat_history",
    output_messages_key="answer",
)
```

---

## ğŸ—‚ï¸ Project Structure

```bash
conversational-rag-pdf-chat/
â”‚
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€
